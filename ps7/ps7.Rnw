\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\pagenumbering{arabic}		
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
 
\pagestyle{fancy}
\fancyhf{}


\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

\title{Problem Set 7}
\author{Alexander Brandt\\SID: 24092167}
\date{November 16 2015 (Extension Request E-mail Included)}

\begin{document}

\maketitle

Friendly Collaborators: Milos Atz, Alex Ojala and Guillame Baquiast.

\section{}

\begin{enumerate}

\item What are the goals of their simulation study and what are the metrics do they consider in assessing their method?
\\\\
The purpose of the simulation study is to assess the accuracy of the proposed asymptotic approximation in finite samples and to examine the power of the EM test, in determining the number of constituient normal distributions in a data set, by using SLC activity in RBCs, adulteration in wine production, and differential gene expression in cancerous and healthy patients.  Type I error and the power are the metrics considered.

\item What choices did the authors have to make in designing their simulation study?  What are the key aspects of the data generating mechanism that likely affect the statistical power of the test?
\\\\
The authors had a choice of mixing fractions, number of normal distributions to inclue (as well as their means and variances), and the number of data points.  Sample size has obvious implications on statstical power, but ``distinct-ness'' (based one mean and variance), as well as the mixing fraction, have implications as well.

\item Suggest some alternatives to how the authors designed their study.  Are there data-generating scenarios that they did not consider that would be useful to consider?
\\\\
Perhaps the ability to fit various skewed normal distributions.  Or very overlapped distributions.

\item Give some thoughts on how to set up a simulation study for their problem that uses principles of basic experimental design, or if you think it would be difficult, say why.
\\\\
How does the model perform under different distributions?  How robust is the model w/ respect to breaking assumption.  Be more ambitious with respect to the mixing fraction.

\item Do their figures/tables do a good job of presenting the simulation results and do you have any alterantive suggestions for how to do this?  Do the authors address the issue of simulation uncertainty/simuilation statndard errors and/or do they convince the reader they've done enough simulation replications?
\\\\
I think the presentation is well done, but label your axes, please.  The tables showing the various p-values for different m = 2 or 3 are especially effective.  The box plots are a nice touch as well.

\item Interpret their tables on power (Tables 4 and 6) - do the results make sense in terms of how the power varies as a function of the data generating mechanism?
\\\\
The EM values increase with iteration, which is pretty much assured by the algorithm.  It'd be nice to know how the mixing fraction differences.  I'm not sure if the dimensionality of the tests lends itself especially well to table formats...

\item Discuss the extent to which they follow JASA's guidelines on simulation studies (see the end of the Unit 10 class notes for the JASA guidelines).
\\\\
I would like to see more discussion about the accuracy (perhaps using a real dataset?).  There is no discussion of pseudorandom-number generation, numerical algorithms, or computers.  Programming languages and major software components are discussed.  P.S. Dude, where's my code?  That should always be a criterea for publishing...

\end{enumerate}

\section{}

\subsection{}

The number of multiplications is \( \sum_{n=2}^N (-n^2 + n(N+1)-1) \), which simplifies, for the first two terms, to:

\[ Ops \approx \frac{1}{6} n^3 + \frac{1}{2} n^2 \]

This fits with Chris' notes.\\

Guidence/Confirmation Citation: Floating Point Operations in Matrix-Vector Calculus (v 1.3), Raphael Hunger. 2007.

\subsection{}

You can overwrite as you go along.  This is because the Cholesky decomposition works by moving ``down" the matrix (i.e. never refers to the preceeding rows).  The animation Chris showed in class (or maybe I just saw it on The YouTube trying to figure out part a)?) provides a graphical intution about this.

\subsection{}

<<cache=TRUE>>=
library(pryr)
# Initialize our arrays for later plotting
sizes <- seq(500,3000,by=250)
memories <- rep(0,length(sizes))
times <- rep(0,length(sizes))

for (i in seq(length(sizes)))
{
    # Construct the matrix
    X <- crossprod(matrix(rnorm(sizes[i]^2), sizes[i]))
    # Find out memory use before
    before <- mem_used()
    # Microbenchmark our cholesky decomposition
    times[i] <- system.time(cX <- chol(X))[3]
    # Find out memory use after
    memories[i] <- mem_used() - before
    rm(cX,X) # To clear before the next usage!
}
plot(x=sizes,y=memories,xlab="n Size",ylab="Memory Usage (Bytes)",main="Memory Usage vs. Matrix Size")
plot(x=sizes,y=times,xlab="n Size",ylab="Time (msec)",main="Operation Time vs. Matrix Size")
@

The values are scaling about \(O(n^3)\) which is roughly what we expect given the algorithm.

\section{}

<<cache=TRUE>>=
set.seed(0)
options(digits=22)
N <- 5000
# Initialize the pos-def matrix
X <- crossprod(matrix(rnorm(N^2), N)) 
y <- rnorm(N) 

print("For the solve(X), then %*% method...")
system.time(v1 <- solve(X) %*% y)
print("For the solve(X,y)...")
system.time(v2 <- solve(X,y))
print("For the Cholesky decomposition...")
system.time(
  {
    # Cholesky decompose, and then backsolve
    U <- chol(X);
    v3 <- backsolve(U, backsolve(U, y, transpose=TRUE));
  }
)

print("Checking that our values are reasonable (i.e., the same)...")
v1[1:10]
v2[1:10]
v3[1:10]
@

...Cholesky is defintely the fastest!  a) \(O(n^3)\) b) \( 1/3 n^3 + O(n^2) \) c) \(1/6 n^3 + O(n^2) \)  (source: the notes).  The timings generally agree.

\subsection{}

<<cache=TRUE>>=
library(MASS)
print("Numerical differences between our answers")
# Max of differences between methods 1, 2 and 2, 3 and 3, 1 respectively
max(abs(v1-v2))
max(abs(v2-v3))
max(abs(v3-v1))

a <- norm(X,type="2") * norm(ginv(X),type="2")
print("The Condition Number")
print(a * 1e-16)
@

The differences are really not that pronounced.  This agrees with the relative numerical stability assured by the condition number.

\section{}

The general scheme and pseudocode for the matrix manipulations used was as follows:\\

Given:

\[ \hat{\beta} = \left( X^T \Sigma^{-1} X \right)^{-1} X^T \Sigma^{-1} Y \]

We use eigenvectors and eigenvalues to rewrite sigma inverse as:

\[ \Sigma^{-1} = C \Lambda C^{-1} \]

Which we can further simply to:

\[ \Sigma^{-1} = P P^T \]

Where:

\[ P = C \Lambda^{1/2} \]

This allows us to transform X and Y, so the QR decomposition can be performed, so that our calculation is of an acceptable speed:

\[X_* = P^T X\]

and

\[Y_* = P^T Y \]

Now we follow a QR decomposition, much like in the notes with OLE:

\[X_*^T X_* \hat \beta = X_*^T Y \]

\[R^T Q^T Q R \hat \beta = R^T Q^T Y_*\]

And we can use back solving to find:

\[ R \hat \beta = Q^T Y_* \]

<<cache=TRUE>>=
library(MASS)
n <- 50
p <- 10

X <- matrix(runif(n*p), ncol=p) 
Sigma <- crossprod(matrix(rnorm(n^2), n)) 
Y <- matrix(runif(n), ncol=1) 

my_gls <- function(X, Sigma, Y, n, p) {
    # Generate sigma inverse
    Sigma_inv <- ginv(Sigma)
    # Eigendecompose sigma inverse
    Sigma_inv_eigen_object <- eigen(Sigma_inv)
    P <- Sigma_inv_eigen_object$vectors %*%
      diag(sqrt(Sigma_inv_eigen_object$values))
    Pt <- t(P)
    # Generate our Y/X "stars" form the pseudocode
    Y_mod <- Pt %*% Y
    X_mod <- Pt %*% X
    # Perform a QR decomposition
    X_mod.qr <- qr(X_mod)
    Q <- qr.Q(X_mod.qr)
    R <- qr.R(X_mod.qr)
    # Backsolve, and return the values
    return(backsolve(R, t(Q) %*% Y_mod))
}

print(my_gls(X, Sigma, Y, n, p))

# Checking our answer against the "long" way...
# This was the problem statement (but too slow for our solution)

Sigma_inv <- ginv(Sigma)
ginv(t(X) %*% Sigma_inv %*% X) %*% t(X) %*% Sigma_inv %*% Y
@

\section{}

\subsection{}

Suppose we write X as:

\[ X = U_{n x n} S_{n x p} V^T_{p x p} \]

Manipulate further:

\begin{align*}
X^T &= (USV^T)^T\\
    &= VS^TU^T\\
X^TX &= VS^TU^TUSV^T\\
     &= VS^TISV^T
\end{align*}

S is diagonalizable, so \(S^T = S\):

\[ X^TX = VS^2V^T \]

So our eigenvectors are \(V\) and \(V^T\) -- the right singular vecotrs of X.  We can also see that \(S^2\) are the eigenvalues -- the square of the singular values of X.\\\\

We now show that \(XX^T\) is positive semidefinite.  Take v s.t. \(v_i \neq 0 \forall i\):

\begin{align*}
v^t X^T X v &= (vX)^T X v\\
            &= s^T s\\
            &= \sum s_i^2 \ge 0
\end{align*}

Sources: Dug through some econometrics notes as well as a linear algebra textbook my roommate had lying around.

<<cache=TRUE>>=
n <- 1000
p <- 500

X <- matrix(runif(n*p), ncol=p) 
eigen_object <- eigen(t(X) %*% X)
X_svd <- svd(X)

# Are the right singular vectors of X equal to
# the eigenvectors of X^t * X?

print("Right Singular Vectors of X...")
head(X_svd$v[,1:10])
print("Eigenvectors of X^t * X")
head(eigen_object$vectors[,1:10])

# Are the eigenvalues of X^t * X equal to the 
# squares of the singular values of X?

print("Eigenvalues of X^t * X")
head(eigen_object$values)
print("Squared singular values of X")
head(X_svd$d**2)

# Is X' * X a positive semidefinite matrix?

library("matrixcalc")
print("Is X' * X a positive semidefinite matrix?")
is.positive.semi.definite(t(X) %*% X)
@

\subsection{}

Now we consider an n x n positive semi-definite matrix X and assume we have already computed the eigendecomposition.
\begin{align*}
XDX^T + Ic &= XDX^T + XcX^T\\
           &= X(D + cI)X^T
\end{align*}

Some code illustrating our proof(s):

<<cache=TRUE>>=
N <- 10
X <- crossprod(matrix(rnorm(N^2), N))
# Store our eigenvalues for later
eigen_objects <- eigen(X)

# Create Z from X and cI
Z <- X + diag(N) * rep(2,10)
Z_eigen_objects <- eigen(Z)

# So slow!  Our "truth" to compare against
Z_eigen_objects$values

# Order O(n)... much faster!
eigen_objects$values + rep(2,10)
@

\end{document}